# Performance & Scalability Initiatives

| Initiative | Why / Decision | Scope & Approach | ETA (working days, 1 dev) | Expected user-facing latency impact |
| --- | --- | --- | --- | --- |
| Supervisor true streaming (Cerebras `astream_events`) | Current “streaming” waits for full completion and replays text, so users see first token after the turn finishes. Decision: move to real streaming in the supervisor path only, keeping existing guardrails per chunk. | Extend `SafeChatCerebras` to expose `astream_events`; update the supervisor node to consume the stream and emit `token.delta` as tokens arrive; keep tool/handoff flow intact; sanitize per chunk and finalize with `message.completed`. No subagent changes in this ticket. | 1 | The supervisor node currently shows ~0.96s p50 and ~4.36s p90 before first words. Streaming should bring first-token latency closer to the model start (around ChatCerebras ~0.55–0.7s) and make long answers flow continuously instead of a single burst. |
| Memory writes & profile sync off hot path | Semantic trigger + write + profile sync currently run before the supervisor responds, adding ~0.68s p50 / 2.61s p90 (memory_hotpath) plus occasional spikes. Decision: keep retrieval in-path, push creation/merge/profile-sync to an async worker, accept a few seconds of eventual consistency. | Introduce background job execution for semantic writes and profile sync (idempotent, retried); refactor `_write_semantic_memory` and `_profile_sync_from_memory` to run there; keep on-path personalization via cached context; emit `memory.created/updated` SSE from the worker. | 4–6 | The memory_hotpath node adds ~0.68s p50 / 2.61s p90 today and can spike higher when Bedrock merge/classify is slow. Moving writes off-path should remove that overhead from the request path, so responses start sooner and avoid those spikes. |
| Intent classifier: smalltalk vs supervisor (Cerebras GPT‑20B) | Smalltalk currently takes the supervisor path, adding routing/tooling overhead. Decision: add a classifier to send smalltalk to a lightweight, fast model (Cerebras GPT‑20B, already in testing) with streaming, and send the rest to the existing supervisor. | Insert classifier before memory_context; fast path skips memory_context, uses the light model with streaming and minimal prompt; low-confidence defaults to supervisor; supervisor path unchanged. | 2–3 | Smalltalk currently rides the full pipeline, so first words often land in the 1–4s window depending on routing. The fast path should align first-token latency with the light model’s start (~0.55–0.7s) while keeping complex tasks on the supervised route. |
| Direct-to-subagent path (finance/wealth/goals) with local prompt split and streaming | Today, even user-selected agents route through supervisor; subagents lack direct streaming and local memory/personality injection. Decision: allow user-chosen finance/wealth/goals to bypass supervisor, stream directly, and receive personality + memory context locally (requires prompt split). | Split the supervisor prompt into reusable personality/context fragments; refactor each of finance/wealth/goal agents to accept those fragments, inject memory context locally, and stream SSE directly; add routing to bypass supervisor when the user selects an agent. | 7–10 | The supervisor hop adds ~0.96s p50 / 4.36s p90 before the agent even starts (~1–3s typical). Direct routing removes that hop, so agent responses should begin sooner and stream without the extra supervisory delay. |
| Migrate SSE to AWS AppSync pub/sub | Current SSE transport is local; scalability and reliability would benefit from managed pub/sub. Decision: move event delivery to AppSync. | Design AppSync schema/topics for token deltas, completions, memory, and navigation; implement publishers in supervisor/agents; update clients to subscribe with auth, ordering, and reconnection; support rollout via environments (dev/uat/prod). | 6–8 | AppSync won’t change node p50/p90 directly but should reduce drops/retries under load, keeping perceived latency stable or slightly better and delivering smoother token streams at higher concurrency. |

Execution order: start with supervisor streaming (fast win), follow with the smalltalk-vs-supervisor intent classifier, then move memory writes/profile sync off the hot path to shave latency further. After that, tackle the direct-to-subagent refactor for finance/wealth/goals, and finish with the AppSync pub/sub migration once the upstream producers are streaming correctly.

